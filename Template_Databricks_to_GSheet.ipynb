{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b75b0ed-296e-461c-862e-9503a18494ef",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Title Placeholder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd65a03c-8f45-48bf-a1b8-0863502ea7f1",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# First Section\n",
    "Config, starting variables and query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "f213722e-0475-4427-9d15-c5d8993f27df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Putting this on a separate cell to avoid cleaning variables by the interpreter restart\n",
    "# Ensure gspread is installed\n",
    "try:\n",
    "    import gspread\n",
    "    print(\"gspread is already installed.\")\n",
    "except ImportError:\n",
    "    print(\"gspread is not installed. Installing...\")\n",
    "    %pip install gspread\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "f7593378-f9f6-447f-b70c-5bdee722d9ad",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Config and Starting Variables"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession # Allows the Spark session to work with Databricks DB\n",
    "from datetime import date, datetime, timedelta # Allows working with dates\n",
    "from decimal import Decimal # Allows casting Decimal type to string or float for JSON conversion\n",
    "import pandas as pd # Dataframe manipulation\n",
    "import gspread # Read and write GSheets\n",
    "\n",
    "\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"NamePlaceholder\").getOrCreate()\n",
    "\n",
    "\n",
    "# current_date = date(2024, 5, 31)\n",
    "current_date = datetime.now()\n",
    "\n",
    "# Calculate the first day of the current month\n",
    "first_day_of_current_month = current_date.replace(day=1)\n",
    "\n",
    "# End of Month date of previous month\n",
    "end_of_previous_month = first_day_of_current_month - timedelta(days=1)\n",
    "\n",
    "# For this particular extraction, the date is always the End of Month date of the previous month\n",
    "extract_date = end_of_previous_month.strftime(\"%Y-%m-%d\") # Formating date to 'YYYY-MM-DD'\n",
    "\n",
    "# report_date will be the extract date\n",
    "report_date_string = extract_date\n",
    "\n",
    "\n",
    "#        ################ IMPORTANT!!!!!! ################\n",
    "# Aside from getting a service account to run the Google API for Gsheets, we\n",
    "# need to give editor permission to the service account on the desired GSheet using \n",
    "# the email we can find in the json generated when creating said service account.\n",
    "\n",
    "# Full Path -> /Workspace/Users/username@something.com/folderName/serviceAccountPlaceholder.json\n",
    "# URL -> URL_Placeholder\n",
    "\n",
    "SHEET_ID = 'SheetIDPlaceholder' # Sheet Name Placeholder\n",
    "# WORKSHEET_NAME = 'WorksheetNamePlaceholder'\n",
    "WORKSHEET_ID = IDNumberPlaceholder # Using worksheet ID to avoid error due to name changes of the worksheets\n",
    "\n",
    "creds = gspread.service_account(\"/Workspace/Users/username@something.com/folderName/serviceAccountPlaceholder.json\")\n",
    "\n",
    "spreadsheet = creds.open_by_key(SHEET_ID)\n",
    "# worksheet = spreadsheet.worksheet(WORKSHEET_NAME)\n",
    "worksheet = spreadsheet.get_worksheet_by_id(WORKSHEET_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "efc7ebce-aa33-41e1-b708-014661a7ab24",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Query"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Using f string to declare the query so we can replace '{extract_date}' with the declared value\n",
    "\n",
    "QUERY = f'''\n",
    "\n",
    "SELECT * FROM TableName\n",
    "WHERE date = '{extract_date}'\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8bf07503-cad9-450b-88f5-9828fa71b6e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Second Section\n",
    "This section will query our SQL database to generate the current month data up to the current date.\n",
    "The result will be a Pandas dataframe with dates casted as strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "f093e161-3bfc-4c22-8df2-f3be4c29b8fc",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Second Section"
    }
   },
   "outputs": [],
   "source": [
    "# Specify catalog\n",
    "spark.sql(\"USE CATALOG catalogPlaceholder\")\n",
    "\n",
    "# Running Query and storing results in a Spark DataFrame\n",
    "spark_df_new_data =  spark.sql(QUERY)\n",
    "\n",
    "# Convert Spark DataFrame to Pandas DataFrame (if using Spark)\n",
    "pandas_df_new_data = spark_df_new_data.toPandas()\n",
    "\n",
    "# Convert all date columns in the DataFrame to string format\n",
    "pandas_df_new_data = pandas_df_new_data.applymap(lambda x: x.strftime('%Y-%m-%d') if isinstance(x, date) else x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9240b31f-2c2a-41c4-a3af-28df582442bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Third Section (__NOT IN USE__)\n",
    "In this section, we will prepare the data for an Incremental Load:\n",
    "1) ~~Query the GSheet to retrive the currently registered data~~\n",
    "2) ~~Compare it with the data obtained from our SQL database using the \"report_date\" column~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "3456f715-7327-46b9-9bbc-998a10373ce3",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Third Section"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "############## INCREMENTAL LOAD #################\n",
    "# At the moment, I prefer to load all the available data of the current month to avoid\n",
    "# differences due to retroactive changes but I'll leave the logic for the future\n",
    "#################################################\n",
    "\n",
    "# # Specify the range of columns to read (e.g., 'A:D' for columns A to D)\n",
    "# range_to_read = 'A:H'\n",
    "\n",
    "# # Get data for the specified range\n",
    "# # this will produce a list of lists (each list inside the list will be a row)\n",
    "# gsheet_data = worksheet.get(range_to_read)\n",
    "\n",
    "# # Turning into a dataframe to manipulate data\n",
    "# gsheet_data_df = pd.DataFrame(gsheet_data[1:], columns=gsheet_data[0])\n",
    "\n",
    "# # Filtering df through difference using column \"report_date\" as reference\n",
    "# filtered_df = pandas_df_new_data[~pandas_df_new_data[\"report_date\"].isin(gsheet_data_df[\"report_date\"])]\n",
    "\n",
    "# # Convert Decimal values to float before creating the list of lists\n",
    "# filtered_df = filtered_df.applymap(lambda x: float(x) if isinstance(x, Decimal) else x)\n",
    "\n",
    "# # We will create a list of lists without headers:\n",
    "# list_new_filtered_data = filtered_df.values.tolist()\n",
    "\n",
    "#################################################\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "615420a8-a24a-4b07-9ff7-b027c587ddcc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Fourth Section\n",
    "In this section, we will:\n",
    "1) Check data to write and cast Decimal to float for JSON conversion\n",
    "2) Turn the df into a list of lists without headers\n",
    "3) Find the row that match the \"report_date\" or first empty row <- IMPORTANT! 'YYYY-MM-DD'\n",
    "4) Write to the GSheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d6da86a-f63c-49ba-86b0-3298f7201e93",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Convert Decimal values to float before creating the list of lists\n",
    "pandas_df_new_data = pandas_df_new_data.applymap(lambda x: float(x) if isinstance(x, Decimal) else x)\n",
    "\n",
    "# We will create a list of lists (required to convert to JSON) without headers\n",
    "list_new_data = pandas_df_new_data.values.tolist()\n",
    "\n",
    "# /////////////////////NOTE////////////////////\n",
    "# Due to data being small enough, I'll just overwrite the data everytime it loads instead of\n",
    "# doing batch updates\n",
    "\n",
    "# # Get all values from column \"A\" (assuming column \"A\" is \"report_date\" and\n",
    "# # contains dates in 'YYYY-MM-DD' format)\n",
    "# col_values = worksheet.col_values(1)\n",
    "\n",
    "# # Find the row index of the report_date\n",
    "# if report_date_string in col_values:\n",
    "#   # Add 1 because GSheets is 1-indexed and list indexing is 0-indexed\n",
    "#   row_index = col_values.index(report_date_string) + 1\n",
    "# else: # If we don't get a match then first empty row\n",
    "#   row_index = len(col_values) + 1\n",
    "\n",
    "\n",
    "# Write the list of lists to the Google Sheet starting on the cell that match\n",
    "# the first day of the current month or the first empty row.\n",
    "\n",
    "# raw=\"false\" will simulate manual input in the GSheet\n",
    "# worksheet.update(list_new_data, f\"A{row_index}\", raw=\"false\") # NOTE: uncomment this when using batch updates\n",
    "worksheet.update(list_new_data, f\"A2\", raw=\"false\")\n",
    "\n",
    "# Adding an empty row to the end so we can append new data without hitting\n",
    "# the \"exceed grid limit\" error\n",
    "worksheet.add_rows(300)\n",
    "\n",
    "\n",
    "print(\"Work Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "6d8604fd-603d-4b27-ab23-6895362193a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "list_new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "48e836df-2599-4799-9a20-386f5e2ad103",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pandas_df_new_data.info()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "Template_Databricks_to_GSheet",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}